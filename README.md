#Lic gRPC
A backend system for scraping, processing, and serving legal/publication data from official sources like Brazil's DOU (DiÃ¡rio Oficial da UniÃ£o).

---

## âœ¨ Key Features
- **Automated Scraping**: Collect data from predefined government portals
- **ML Filtering**: Transformer-based model to flag relevant publications
- **gRPC API**: Serve filtered data with pagination and time-based queries
- **SQLite Storage**: Persistent storage for publications and metadata
- **Scheduled Tasks**: Daily auto-scraping via configurable scheduler

---

## ğŸ›  Technologies
- **gRPC** (Protocol Buffers)
- **SQLite** + **Peewee ORM**
- **PyTorch** (Transformer model)
- **BeautifulSoup** (HTML parsing)

---

## âš™ï¸ Quick Start

### Prerequisites
- Python 3.10+
- `requirements.txt` dependencies

```bash
# Install dependencies
pip install -r requirements.txt

# Compile Protobuf definitions
./compile_protos.bat

# Start server
python server/main_server.py

# Test client (in separate terminal)
python main_test_client.py
```

---

## ğŸ“š Documentation
| Language      | Link                              |
|---------------|-----------------------------------|
| English       | [Full Documentation](DOC.md)     |
| Portuguese    | [DocumentaÃ§Ã£o Completa](DOC-PT_BR.md) |

---

## ğŸ“‚ Project Structure
```
licscrap/
â”œâ”€â”€ database/       # DB models & operations
â”œâ”€â”€ ml/             # ML model & inference
â”œâ”€â”€ scraper/        # Web scraping components
â”œâ”€â”€ server/         # gRPC server implementation
â””â”€â”€ tests/          # Unit/integration tests
```

---

## ğŸŒŸ Key Workflow
1. **Scraper** collects raw data â†’ **ML Model** filters by relevance
2. **Database** stores processed publications
3. **gRPC Server** serves filtered results to clients
4. **Scheduler** triggers daily auto-scraping

